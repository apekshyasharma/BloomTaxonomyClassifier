{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51e6a0c6",
      "metadata": {
        "id": "51e6a0c6"
      },
      "source": [
        "# Project Title & Overview\n",
        "\n",
        "This notebook implements a supervised text classification pipeline to automatically classify educational questions into Bloom's Taxonomy cognitive levels using a Multinomial Logistic Regression model. The workflow includes data loading, preprocessing, TF-IDF feature extraction, model training, and evaluation using standard classification metrics.\n",
        "\n",
        "The Bloom's Taxonomy levels considered are: **Remembering**, **Understanding**, **Applying**, **Analyzing**, **Evaluating**, and **Creating**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c683014",
      "metadata": {
        "id": "0c683014"
      },
      "source": [
        "# Importing Required Libraries\n",
        "\n",
        "This section installs and imports all libraries required to run the notebook in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "507bd4bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "507bd4bf",
        "outputId": "1cca75a6-49c7-4cea-b3b5-9e39f7c4ce48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f0176ada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0176ada",
        "outputId": "c357c632-0274-4a90-ced7-a05f42feea79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Set global visual style and random seed for reproducibility\n",
        "sns.set(style='whitegrid')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e109d84",
      "metadata": {
        "id": "4e109d84"
      },
      "source": [
        "# Dataset Loading\n",
        "\n",
        "In this section, the Bloom's Taxonomy dataset is uploaded and loaded from a CSV file. The dataset is expected to contain the following columns:\n",
        "\n",
        "- `question`: the educational question text\n",
        "- `label`: the corresponding Bloom's Taxonomy level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2f11b089",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f11b089",
        "outputId": "d7ba60d5-5d0e-4a63-d64d-f4035023f07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c58d5671",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c58d5671",
        "outputId": "07878935-e0ea-4809-c71e-107d19b52fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                           Questions Category\n",
            "0  About what proportion of the population of the...      BT1\n",
            "1  Correctly label the brain lobes indicated on t...      BT1\n",
            "2                          Define compound interest.      BT1\n",
            "3                  Define four types of traceability      BT1\n",
            "4                               Define mercantilism.      BT1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/dataset/blooms_taxonomy_dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8021749",
      "metadata": {
        "id": "f8021749"
      },
      "source": [
        "# Exploratory Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9def9f",
      "metadata": {
        "id": "bf9def9f"
      },
      "outputs": [],
      "source": [
        "print('Columns:', df.columns.tolist())\n",
        "\n",
        "print('\\nClass distribution (label counts):')\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Compute question length in number of tokens (approximate, based on whitespace)\n",
        "df['question_length'] = df['question'].astype(str).str.split().apply(len)\n",
        "\n",
        "print('\\nQuestion length summary (in tokens):')\n",
        "print(df['question_length'].describe())\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cab9ba7",
      "metadata": {
        "id": "2cab9ba7"
      },
      "outputs": [],
      "source": [
        "# Plot class distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(x='label', data=df, order=sorted(df['label'].unique()))\n",
        "plt.title(\"Distribution of Bloom's Taxonomy Levels\")\n",
        "plt.xlabel(\"Bloom's Level\")\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot distribution of question lengths\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(df['question_length'], bins=30, kde=True)\n",
        "plt.title('Distribution of Question Lengths (in Tokens)')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14fda3f6",
      "metadata": {
        "id": "14fda3f6"
      },
      "source": [
        "# Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21493f5e",
      "metadata": {
        "id": "21493f5e"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Preprocess a question string by normalizing, tokenizing,\n",
        "    removing stopwords and punctuation, and lemmatizing tokens.\"\"\"\n",
        "    # Ensure input is a string and convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha() and token not in stop_words:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            cleaned_tokens.append(lemma)\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Apply preprocessing to all questions\n",
        "df['cleaned_question'] = df['question'].astype(str).apply(preprocess_text)\n",
        "\n",
        "df[['question', 'cleaned_question']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ce46c3",
      "metadata": {
        "id": "c1ce46c3"
      },
      "source": [
        "# Feature Extraction using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf3e111",
      "metadata": {
        "id": "9cf3e111"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "tfidf_vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd4f880",
      "metadata": {
        "id": "1fd4f880"
      },
      "source": [
        "# Train–Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27eb1db",
      "metadata": {
        "id": "d27eb1db"
      },
      "outputs": [],
      "source": [
        "# Encode Bloom's Taxonomy labels numerically\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Use the preprocessed text as input features\n",
        "X_text = df['cleaned_question'].values\n",
        "\n",
        "# Train–test split with stratification\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Fit TF-IDF only on the training data and transform both splits\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
        "\n",
        "print('Training TF-IDF shape:', X_train_tfidf.shape)\n",
        "print('Test TF-IDF shape:', X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75320503",
      "metadata": {
        "id": "75320503"
      },
      "source": [
        "# Multinomial Logistic Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d46295",
      "metadata": {
        "id": "52d46295"
      },
      "outputs": [],
      "source": [
        "log_reg_model = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('Model training complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359dbd2c",
      "metadata": {
        "id": "359dbd2c"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677ea9d9",
      "metadata": {
        "id": "677ea9d9"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test_tfidf)\n",
        "\n",
        "# Compute evaluation metrics \n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print('Evaluation Metrics (Macro-Averaged):')\n",
        "print(f'Accuracy:  {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall:    {recall:.4f}')\n",
        "print(f'F1-score:  {f1:.4f}')\n",
        "\n",
        "# Detailed per-class report\n",
        "class_names = label_encoder.classes_\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "# Confusion matrix visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix for Bloom's Taxonomy Classification\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
